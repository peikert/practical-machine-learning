dTreePrediction
testingSet
partition <- createDataPartition(y=pml_training_cleaned[,classe_index_column], p=0.75, list=FALSE)
trainingSet <- pml_training_cleaned[partition,]
testingSet <- pml_training_cleaned[partition,]
testing20cases <- pml_testing[,unlist(accelerometers_cols)]#[-partition,]
dim(trainingSet)
dim(testingSet)
dim(testing20cases)
dim(trainingSet)
dTreeModel <- train(classe ~ ., data = trainingSet, method= 'rpart')
plot(dTreeModel$finalModel, uniform = TRUE, main="Classification Tree")
text(dTreeModel$finalModel, use.n=TRUE, all= TRUE, cex=0.8)
fancyRpartPlot(dTreeModel$finalModel)
dTreePrediction <- predict(dTreeModel$finalModel,testingSet,type='class')
dTreeValidation <-  confusionMatrix(dTreePrediction, testingSet$classe)
dTreeValidation
dTreeValidation$table
attributes(dTreeValidation)
attri(dTreeValidation,"overall")
attr(dTreeValidation,"overall")
attr(dTreeValidation,"overall")
attr(dTreeValidation,"names")
attr(dTreeValidation,"positive")
attr(attr(dTreeValidation,"names"),'overall')
attr(dTreeValidation,"names")
dTreeValidation
dTreeValidation$overall['Accuracy']
dTreePrediction <- predict(dTreeModel$finalModel,testingSet,type='class')
dTreeValidation <-  confusionMatrix(dTreePrediction, testingSet$classe)
dTreeValidation
dTreeValidation$overall['Accuracy']
rForestModel <- train(classe ~ ., data = trainingSet, method= 'rf', prox=FALSE)
rForestModel <- train(classe ~ ., data = trainingSet, method= 'rf', prox=TRUE)
classe_index_column
library("randomForest")
rForestModel <- randomForest(
x=trainingSet[, -classe_index_column],
y=trainingSet$classe,
xtest=testingSet[, -classe_index_column],
ytest=testingSet$classe,
ntree=100,
keep.forest=TRUE,
proximity=TRUE)
rForestModel
rForestModelAcc <- round(1-sum(rForestModel$confusion[, 'class.error']),3)
rForestModelAcc
rForestValidationAcc <- round(1-sum(rForestModel$confusion[, 'class.error']),3)
rForestValidationAcc
dTreeValidationAcc <- dTreeValidation$overall['Accuracy']
dTreeValidationAcc
dTreeValidationAcc <- as.numeric(dTreeValidation$overall['Accuracy'])
dTreeValidationAcc
rForestValidationAcc
rForestPrediction <- predict(rForestModel,testing20cases)
rForestPrediction
saveRDS(rForestModel,"rForestModel_randomForest.RDS")
rForestModel <- train(classe ~ ., data = trainingSet, method= 'rf', prox=TRUE)
saveRDS(rForestModel,"rForestModel_train.RDS")
rForestModel
rForestPrediction <- predict(rForestModel,testingSet)
dTreeValidation <-  confusionMatrix(rForestPrediction, testingSet$classe)
dTreeValidation
round(1-sum(rForestModel$confusion[, 'class.error']),3)
rForestValidationAcc_train <- round(1-sum(rForestModel$confusion[, 'class.error']),3)
ldaModel <- train(classe ~ ., data = trainingSet, method= 'lda')
saveRDS(ldaModel,"ldaModel.RDS")
nbModel <- train(classe ~ ., data = trainingSet, method= 'nb')
saveRDS(nbModel,"nbModel.RDS")
warnings()
ldaPrediction <- predict(ldaModel,testingSet)
nbPrediction <- predict(nbModel,testingSet)
boostingModel <- train(classe ~ ., data = trainingSet, method= 'gbm', verbose=FALSE)
saveRDS(boostingModel,"boostingModel.RDS")
boostingPrediction <- predict(boostingModel,testingSet)
print(boostingModel)
boostingValidation <-  confusionMatrix(boostingPrediction, testingSet$classe)
boostingValidation
round(1-sum(boostingModel$confusion[, 'class.error']),3)
round(1-sum(boostingValidation$confusion[, 'class.error']),3)
boostingValidation$confusion
boostingValidation
boostingValidation$confusion
boostingModel$confusion
rForestModel$confusion
rForestModel$confusion
rForestModel
rForestModel
rForestModel$results
rForestModel$bestTune
rForestModel$results$mtry == rForestModel$bestTune
rForestModel$bestTune
rForestModel$results$mtry == as.numberic(rForestModel$bestTune)
rForestModel$results$mtry == as.numeric(rForestModel$bestTune)
rForestModel$results$Accuracy[rForestModel$results$mtry == as.numeric(rForestModel$bestTune)]
rForestModel <- readRDS("rForestModel_randomForest.RDS"))
rForestValidationAcc_randomForest <- rForestModel$results$Accuracy[rForestModel$results$mtry == as.numeric(rForestModel$bestTune)]
rForestModel <- readRDS("rForestModel_randomForest.RDS")
rForestValidationAcc_randomForest <- rForestModel$results$Accuracy[rForestModel$results$mtry == as.numeric(rForestModel$bestTune)]
rForestValidationAcc_randomForest
rForestModel <- readRDS("rForestModel_randomForest.RDS")
rForestModel
rForestModel$confusion
rForestModel$confusion[,'class.error']
sum(rForestModel$confusion[,'class.error'])
1-sum(rForestModel$confusion[,'class.error'])
rForestValidationAcc_randomForest <- 1-sum(rForestModel$confusion[,'class.error'])
rForestValidationAcc_randomForest
boostingModel
boostingValidation
dTreeValidation
rForestPrediction <- predict(rForestModel,testing20cases)
rForestPrediction <- predict(rForestModel,testingSet)
rForestValidation <-  confusionMatrix(rForestPrediction, testingSet$classe)
rForestValidation
rForestModel <- readRDS("rForestModel_train.RDS")
rForestPrediction <- predict(rForestModel,testingSet)
rForestValidation <-  confusionMatrix(rForestPrediction, testingSet$classe)
rForestValidation
rForestPrediction20cases <- predict(rForestModel,testing20cases)
rForestPrediction20cases
trainCtrl <- trainControl(method   =   "repeatedcv",
repeats   =   3,
number = 10,
classProbs   =   TRUE,
savePredictions = 'final',
summaryFunction   =   twoClassSummary)
nbModel <- train(classe ~ ., data = trainingSet,
method   =   'nb',
trControl   =   trainCtrl,
metric   =   "ROC", #for classification
preProc = c("center", "scale","conditionalX"),
tuneLength = 10)
nbModel <- train(classe ~ ., data = trainingSet,
method   =   'nb',
trControl   =   trainCtrl,
metric   =   "ROC", #for classification
preProc = c("center", "scale"),
tuneLength = 10)
install.packages('pROC')
nbModel <- train(classe ~ ., data = trainingSet,
method   =   'nb',
trControl   =   trainCtrl,
metric   =   "ROC", #for classification
preProc = c("center", "scale"),
tuneLength = 10)
library('pROC')
remove.packages('pROC')
install.packages('pROC')
library('pROC')
devtools::install_github("xrobin/pROC", build_vignettes = TRUE)
devtools::install_github("xrobin/pROC", build_vignettes = TRUE)
install.packages('Rcpp')
install.packages('Vennerable')
library(reshape)
devtools::install_github("RcppCore/Rcpp", build_vignettes = TRUE)
remove.packages('Rcpp')
install.packages('Rcpp')
library('reshape')
remove.packages('Rcpp')
install.packages('Rcpp')
library('Rcpp')
print('test')
library('Rcpp')
# install.packages('Rcpp')
library('Rcpp')
knitr::opts_chunk$set(echo = TRUE)
library("caret")
library("corrplot")
library("Hmisc")
library("FactoMineR")
library("stringr")
library("factoextra")
set.seed(20180719)
# install.packages('pROC') # broken
# devtools::install_github("xrobin/pROC", build_vignettes = TRUE)
# install.packages('Rcpp')
library('Rcpp')
# devtools::install_github("RcppCore/Rcpp", build_vignettes = TRUE)
library('reshape')
install.packages('reshape')
datasetColumnIntersection <- intersect(colnames(pml_training),colnames(pml_testing))
datasetColumnOutersection <- sort(c(setdiff(colnames(pml_training), colnames(pml_testing)),
setdiff(colnames(pml_testing), colnames(pml_training))))
datasetColumnIntersection
accelerometers <- c("belt", "forearm", "arm","dumbbell")
accelerometers_cols  <- lapply(accelerometers,function(x)colnames(pml_training)[grep(paste0('([^A-Za-z]|^)',x,'([^A-Za-z]|$)'),colnames(pml_training))])
accelerometers_cols
datasetColumnIntersection
datasetColumnOutersection <- sort(c(setdiff(colnames(pml_training), colnames(pml_testing)),
setdiff(colnames(pml_testing), colnames(pml_training))))
datasetColumnOutersection
accelerometers
accelerometers_cols
accelerometers_cols_temp <- accelerometers_cols
for(n in names(accelerometers_cols_temp)){
accelerometers_cols_temp[[n]] <- sapply(accelerometers_cols_temp[[n]], function(x){
m <- str_match(x,paste0('(.*)','_(',n,')','([^A-Za-z\\d]{0,1})','(.*)'))
if(any(!is.na(m)))paste(m[c(2)],collapse = '')
})
}
# identifcation columns that can be found for all four instruments
accelerometers_cols_temp <- accelerometers_cols
for(n in names(accelerometers_cols_temp)){
accelerometers_cols_temp[[n]] <- sapply(accelerometers_cols_temp[[n]], function(x){
m <- str_match(x,paste0('(.*)','_(',n,')','([^A-Za-z\\d]{0,1})','(.*)'))
if(any(!is.na(m)))paste(m[c(2)],collapse = '')
})
}
v <- Vennerable::Venn(accelerometers_cols_temp)
intersectionSets <- attr(v,'IntersectionSets')
accelerometers_cols_temp
cols
cols <- unique(unlist(sapply(intersectionSets$`1111`,function(x)colnames(pml_training)[startsWith(colnames(pml_training),x)])))
cols
# identification of columns containing less missing values
validValuesPerCol <-  apply(pml_training_cleaned[unlist(accelerometers_cols_clean)],2,function(x)sum(!is.na(x) & is.numeric(x)))
h1  <- hist(featureCor, main = 'Correlation of features')
h1  <- hist(featureCor, main = 'Correlation of features',xlab='Correlation')
h1
cat('Howevery the model have only a weak accuracy (',dTreeValidationAcc,')')
cat('The random forest model trained by the carrot::train function results in a accuracy of',rForest_trainValidationAcc,'.')
randomForest
boostingValidationAcc
accuracy_list
# Chunk 1: setup
knitr::opts_chunk$set(
echo = TRUE
,cache = TRUE
,warning = FALSE
,message = FALSE
,cache.lazy = FALSE
)
library("caret")
library("corrplot")
library("Hmisc")
library("FactoMineR")
library("stringr")
library("factoextra")
library('rattle')
library("randomForest")
set.seed(20180719)
# Chunk 2: loading data
pml_training <-
read.csv(
"data/pml-training.csv",
sep = ',',
header = TRUE,
row.names = NULL,
check.names = FALSE,
stringsAsFactors = FALSE
)
pml_testing <-
read.csv(
"data/pml-testing.csv",
sep = ',',
header = TRUE,
row.names = NULL,
check.names = FALSE,
stringsAsFactors = FALSE
)
# dim(pml_training)
# dim(pml_testing)
# str(pml_training)
# str(pml_testing)
colnames(pml_training)[1] <- 'id_row'
colnames(pml_testing)[1] <- 'id_row'
pml_training$id_row <- NULL
pml_testing$id_row <- NULL
# Chunk 3: Typo crrection
colnames(pml_training) <- sapply(colnames(pml_training),function(x)str_replace(x,'picth','pitch'))
colnames(pml_testing) <- sapply(colnames(pml_testing),function(x)str_replace(x,'picth','pitch'))
# Chunk 4: cleaning
# compare valible cols in train and test set
datasetColumnIntersection <- intersect(colnames(pml_training),colnames(pml_testing))
datasetColumnOutersection <- sort(c(setdiff(colnames(pml_training), colnames(pml_testing)),
setdiff(colnames(pml_testing), colnames(pml_training))))
# assignment of columns to there belonging measuring instruments
accelerometers <- c("belt", "forearm", "arm","dumbbell")
accelerometers_cols  <- lapply(accelerometers,function(x)colnames(pml_training)[grep(paste0('([^A-Za-z]|^)',x,'([^A-Za-z]|$)'),colnames(pml_training))])
names(accelerometers_cols) <- accelerometers
# identifcation columns that can be found for all four instruments
accelerometers_cols_temp <- accelerometers_cols
for(n in names(accelerometers_cols_temp)){
accelerometers_cols_temp[[n]] <- sapply(accelerometers_cols_temp[[n]], function(x){
m <- str_match(x,paste0('(.*)','_(',n,')','([^A-Za-z\\d]{0,1})','(.*)'))
if(any(!is.na(m)))paste(m[c(2)],collapse = '')
})
}
v <- Vennerable::Venn(accelerometers_cols_temp)
intersectionSets <- attr(v,'IntersectionSets')
cols <- unique(unlist(sapply(intersectionSets$`1111`,function(x)colnames(pml_training)[startsWith(colnames(pml_training),x)])))
accelerometers_cols_clean  <- lapply(accelerometers,function(x)cols[grep(paste0('([^A-Za-z]|^)',x,'([^A-Za-z]|$)'),cols)])
names(accelerometers_cols_clean) <- accelerometers
# cleaning columns and converting features to numeric
pml_training_cleaned <- pml_training
pml_training_cleaned[unlist(accelerometers_cols_clean)] <- sapply(pml_training_cleaned[unlist(accelerometers_cols_clean)],as.numeric)
# validValuesPerRow <- apply(pml_training_cleaned[unlist(accelerometers_cols_clean)],1,function(x)sum(!is.na(x) & is.numeric(x)))
# table(validValuesPerRow)
# identification of columns containing less missing values
validValuesPerCol <-  apply(pml_training_cleaned[unlist(accelerometers_cols_clean)],2,function(x)sum(!is.na(x) & is.numeric(x)))
table(validValuesPerCol)
valid_cols <- sort(names(validValuesPerCol)[validValuesPerCol == max(as.numeric(names(table(validValuesPerCol))))])
for(n in names(accelerometers_cols)){
accelerometers_cols[[n]] <- accelerometers_cols[[n]][accelerometers_cols[[n]] %in% valid_cols]
}
# reducing the columns to the bedefined set
# dim(pml_training_cleaned)
pml_training_cleaned <- pml_training_cleaned[c('classe',unlist(accelerometers_cols))]
pml_training_cleaned <- pml_training_cleaned[complete.cases(pml_training_cleaned),]
# dim(pml_training_cleaned)
classe_index_column <- 1
# converting classe column to factor
classes <- pml_training_cleaned$classe
classes <- as.factor(classes)
levels(classes) <- 1:length(levels(classes))
# classes <- as.numeric(classes)
pml_training_cleaned$classe <- classes
# test for near zero variance of columns
nzv_control <- nearZeroVar(pml_training_cleaned[,-1])
`%ni%` <- Negate(`%in%`)
# Chunk 5: Creation to Training Sets
partition <- createDataPartition(y=pml_training_cleaned[,classe_index_column], p=0.75, list=FALSE)
trainingSet <- pml_training_cleaned[partition,]
testingSet <- pml_training_cleaned[partition,]
testing20cases <- pml_testing[,unlist(accelerometers_cols)]
# dim(trainingSet)
# dim(testingSet)
# dim(testing20cases)
# Chunk 6: Correlation
featureCor <- abs(cor(subset(trainingSet,select = names(trainingSet) %ni% 'classe'),as.numeric(trainingSet[,classe_index_column])))
cat("In the histogram of correlation you can the that the features obviously only show weak correlation.")
h1  <- hist(featureCor, main = 'Correlation of features',xlab='Correlation')
show(h1)
bestCorrelations <- subset(as.data.frame(as.table(featureCor)), abs(Freq)>0.3)
coi <- rownames(featureCor)[which(featureCor > 0.20)]
cat("In a strip featurePlot we can see that all feature show the same distribution among the 5 classe's")
f1 <- featurePlot(x = subset(trainingSet,select = coi),
y = trainingSet[,classe_index_column],
plot = "strip"
)
show(f1)
cat("By the pairs featurePlot of the highest correlating features it is not possible to identify a clear feature for classification")
f2 <- featurePlot(x = subset(trainingSet,select = coi),
y = trainingSet[,classe_index_column],
plot = "pairs"
)
show(f2)
correlationMatrix <- cor(subset(trainingSet,select = names(trainingSet) %ni% 'classe'))
correlationMatrix <-  abs(correlationMatrix)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.8)
excludeColumns <- c(highlyCorrelated, classe_index_column)
c1 <- corrplot(correlationMatrix
,method=c("circle")
,type="lower"
# ,order="hclust"
,hclust.method = "ward.D2"
,tl.cex=0.50
,tl.col="black"
,tl.srt = 45
,diag = F
# ,cl.lim=c(-1,1)
,cl.lim=c(0,1)
,addgrid.col = NA
)
cat("Here you can see the plot of correction")
c1
c2 <- corrplot(correlationMatrix[highlyCorrelated,highlyCorrelated]
,method=c("circle")
,type="lower"
,order="hclust"
,hclust.method = "ward.D2"
,tl.cex=0.50
,tl.col="black"
,tl.srt = 45
,diag = F
# ,cl.lim=c(-1,1)
,cl.lim=c(0,1)
,addgrid.col = NA
)
# Chunk 7
df_temp <- subset(trainingSet,select = names(trainingSet) %ni% 'classe')
res.pca = PCA(X = df_temp
,scale.unit = FALSE #  is importent, data are not scale correctly
,ncp = ncol(df_temp) # number of final principal component
,graph = FALSE)
fv1 <- fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
fv2 <- fviz_pca_ind(res.pca, repel = TRUE, label="none", habillage = trainingSet$classe)
# Chunk 8: dTree
# library('rpart')
# library('rpart.plot')
# dTreeModel <- rpart(classe ~ ., data = trainingSet, method= 'class')
# fancyRpartPlot(dTreeModel)
if(F){
dTreeModel <- train(classe ~ ., data = trainingSet, method= 'rpart')
saveRDS(dTreeModel,"dTreeModel.RDS")
}else{
dTreeModel <- readRDS("dTreeModel.RDS")
}
# plot(dTreeModel$finalModel, uniform = TRUE, main="Classification Tree")
# text(dTreeModel$finalModel, use.n=TRUE, all= TRUE, cex=0.8)
fp <- fancyRpartPlot(dTreeModel$finalModel)
cat('Here you can see the result of the trained decision tree!')
show(fp)
dTreePrediction <- predict(dTreeModel$finalModel,testingSet,type='class')
dTreeValidation <-  confusionMatrix(dTreePrediction, testingSet$classe)
dTreeValidation
dTreeValidationAcc <- as.numeric(dTreeValidation$overall['Accuracy'])
cat('Howevery the model have only a weak accuracy (',dTreeValidationAcc,')')
# Chunk 9: random Forest
if(F){
rForestModel_train <- train(classe ~ ., data = trainingSet, method= 'rf', prox=TRUE)#saveRDS(rForestModel,"rForestModel_train.RDS")
}else{
rForestModel_train <- readRDS("rForestModel_train.RDS")
}
rForestModel_train
rForest_trainPrediction <- predict(rForestModel_train,testingSet)
rForest_trainValidation <-  confusionMatrix(rForest_trainPrediction, testingSet$classe)
rForest_trainPrediction20cases <- predict(rForestModel_train,testing20cases)
rForest_trainValidationAcc <- rForestModel_train$results$Accuracy[rForestModel_train$results$mtry == as.numeric(rForestModel_train$bestTune)]
cat('The random forest model trained by the carrot::train function results in a accuracy of',rForest_trainValidationAcc,'.')
if(F){
rForestModel_randomFores <- randomForest(
x=trainingSet[, -classe_index_column],
y=trainingSet$classe,
xtest=testingSet[, -classe_index_column],
ytest=testingSet$classe,
ntree=100,
keep.forest=TRUE,
proximity=TRUE)
saveRDS(rForestModel_randomFores,"rForestModel_randomForest.RDS")
}else{
rForestModel_randomForest <- readRDS("rForestModel_randomForest.RDS")
}
rForest_randomForestPrediction <- predict(rForestModel_randomForest,testingSet)
rForest_randomForestValidation <-  confusionMatrix(rForest_randomForestPrediction, testingSet$classe)
rForest_randomForestPrediction20cases <- predict(rForestModel_randomForest,testing20cases)
rForestrForestModel_randomForestValidationAcc <- 1-sum(rForestModel_randomForest$confusion[,'class.error'])
cat('The random forest model trained by the randomForest::randomForest function results in a accuracy of',rForestrForestModel_randomForestValidationAcc,'.')
saveRDS(rForest_randomForestPrediction20cases,"rForest_randomForestPrediction.RDS") # all correct
# Chunk 10: Linear discriminant Analysis and naive Bayes
if(F){
ldaModel <- train(classe ~ ., data = trainingSet, method= 'lda')
saveRDS(ldaModel,"ldaModel.RDS")
}else{
ldaModel <- readRDS("ldaModel.RDS")
}
# nbModel <- train(classe ~ ., data = trainingSet, method= 'nb')
# saveRDS(nbModel,"nbModel.RDS")
if(F){
trainCtrl <- trainControl(method   =   "repeatedcv",
repeats   =   3,
number = 5,
classProbs   =   TRUE,
savePredictions = 'final',
summaryFunction   =   twoClassSummary)
colnames(trainingSet)
temp_trainingSet <- trainingSet
temp_trainingSet$classe <- as.factor(paste0('c',temp_trainingSet$classe))
levels(temp_trainingSet$classe) <- paste0('c',levels(trainingSet$classe))
nbModel <- train(classe ~ ., data = temp_trainingSet,
method   =   'nb',
# trControl   =   trainCtrl,
metric   =   "ROC", #for classification
preProc = c("center", "scale"),
tuneLength = 10)
nbModel <- train(classe ~ ., data = temp_trainingSet, method= 'nb')
saveRDS(nbModel,"nbModel.RDS")
}else{
nbModel <- readRDS("nbModel.RDS")
}
ldaPrediction <- predict(ldaModel,testingSet)
nbPrediction <- predict(nbModel,testingSet)
compareLdaNb <- table(ldaPrediction,nbPrediction)
ldaValidation <- confusionMatrix(ldaPrediction, testingSet$classe)
temp_testingSet <- testingSet
temp_testingSet$classe <- as.factor(paste0('c',temp_testingSet$classe))
levels(temp_testingSet$classe) <- paste0('c',levels(testingSet$classe))
nbValidation <- confusionMatrix(nbPrediction, temp_testingSet$classe)
ldaModelValidationAcc <- ldaModel$results$Accuracy
nbModelValidationAcc <- nbModel$results$Accuracy[nbModel$results$usekernel==nbModel$bestTune$usekernel]
cat('The linear discriminant analysis Model results in an accuracy of',ldaModelValidationAcc,'and the model of naive bayes results in an accuracy of ',nbModelValidationAcc,'.')
# Chunk 11: Boosting
if(F){
boostingModel <- train(classe ~ ., data = trainingSet, method= 'gbm', verbose=FALSE)
saveRDS(boostingModel,"boostingModel.RDS")
}else{
boostingModel <- readRDS("boostingModel.RDS")
}
boostingPrediction <- predict(boostingModel,testingSet)
print(boostingModel)
boostingValidation <-  confusionMatrix(boostingPrediction, testingSet$classe)
boostingModel
boostingValidationAcc <- boostingModel$results$Accuracy[
boostingModel$results$n.trees ==  boostingModel$bestTune$n.trees &
boostingModel$results$interaction.depth ==  boostingModel$bestTune$interaction.depth
]
cat('The gradient boosting machine achieved an accuracy of',boostingValidationAcc,'.')
# Chunk 12
accuracy_list <- c(
rForest_train = rForest_trainValidationAcc,
rForestrForestModel_randomForest = rForestrForestModel_randomForestValidationAcc,
lda = ldaModelValidationAcc,
nb = nbModelValidationAcc,
boosting = boostingValidationAcc
)
accuracy_list < sort(accuracy_list, decreasing = T)
cat('The random Forest Model performed best of all tested methods.')
accuracy_list[1]
letters <- c('A','B','C','D','E')
final_prediction <- letters[rForest_trainPrediction20cases]
cat('For the quiz the predicted classes are:',final_prediction)
