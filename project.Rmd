---
title: "Practical Machine Learning"
author: "Christian Peikert"
date: "19 Juli 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
echo = TRUE
,cache = TRUE
,warning = FALSE
,message = FALSE
,cache.lazy = FALSE
)
library("caret")
library("corrplot")
library("Hmisc")
library("FactoMineR")
library("stringr")
library("factoextra")
library('rattle')
library("randomForest")
set.seed(20180719)
```

#### Background ####
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

#### Data loading ####
```{r loading data}
pml_training <-
  read.csv(
    "data/pml-training.csv",
    sep = ',',
    header = TRUE,
    row.names = NULL,
    check.names = FALSE,
    stringsAsFactors = FALSE
  )

pml_testing <-
  read.csv(
    "data/pml-testing.csv",
    sep = ',',
    header = TRUE,
    row.names = NULL,
    check.names = FALSE,
    stringsAsFactors = FALSE
  )

# dim(pml_training)
# dim(pml_testing)
# str(pml_training)
# str(pml_testing)

colnames(pml_training)[1] <- 'id_row'
colnames(pml_testing)[1] <- 'id_row'
pml_training$id_row <- NULL
pml_testing$id_row <- NULL

```

The training data set contains `r dim(pml_training)[1]` and `r dim(pml_training)[2]-1` columns.
The column "classe" contain the correct classification which should be achieved.

`r table(pml_training$classe)` 

Class B to E are uniformly represented, however class A has a much higher frequency.

#### Typo crrection ####

```{r Typo crrection}
colnames(pml_training) <- sapply(colnames(pml_training),function(x)str_replace(x,'picth','pitch'))
colnames(pml_testing) <- sapply(colnames(pml_testing),function(x)str_replace(x,'picth','pitch'))
```

Correction of the colname typo 'picth' to 'pitch'

#### Data cleaning ####

```{r cleaning}
# compare valible cols in train and test set
datasetColumnIntersection <- intersect(colnames(pml_training),colnames(pml_testing))
datasetColumnOutersection <- sort(c(setdiff(colnames(pml_training), colnames(pml_testing)),
                                    setdiff(colnames(pml_testing), colnames(pml_training))))

# assignment of columns to there belonging measuring instruments
accelerometers <- c("belt", "forearm", "arm","dumbbell")
accelerometers_cols  <- lapply(accelerometers,function(x)colnames(pml_training)[grep(paste0('([^A-Za-z]|^)',x,'([^A-Za-z]|$)'),colnames(pml_training))])
names(accelerometers_cols) <- accelerometers

# identifcation columns that can be found for all four instruments
accelerometers_cols_temp <- accelerometers_cols
for(n in names(accelerometers_cols_temp)){
  accelerometers_cols_temp[[n]] <- sapply(accelerometers_cols_temp[[n]], function(x){
    m <- str_match(x,paste0('(.*)','_(',n,')','([^A-Za-z\\d]{0,1})','(.*)'))
  if(any(!is.na(m)))paste(m[c(2)],collapse = '')
  })
}
v <- Vennerable::Venn(accelerometers_cols_temp)
intersectionSets <- attr(v,'IntersectionSets')


cols <- unique(unlist(sapply(intersectionSets$`1111`,function(x)colnames(pml_training)[startsWith(colnames(pml_training),x)])))

accelerometers_cols_clean  <- lapply(accelerometers,function(x)cols[grep(paste0('([^A-Za-z]|^)',x,'([^A-Za-z]|$)'),cols)])
names(accelerometers_cols_clean) <- accelerometers


# cleaning columns and converting features to numeric
pml_training_cleaned <- pml_training
pml_training_cleaned[unlist(accelerometers_cols_clean)] <- sapply(pml_training_cleaned[unlist(accelerometers_cols_clean)],as.numeric)

# validValuesPerRow <- apply(pml_training_cleaned[unlist(accelerometers_cols_clean)],1,function(x)sum(!is.na(x) & is.numeric(x)))
# table(validValuesPerRow)

# identification of columns containing less missing values
validValuesPerCol <-  apply(pml_training_cleaned[unlist(accelerometers_cols_clean)],2,function(x)sum(!is.na(x) & is.numeric(x))) 
table(validValuesPerCol)
valid_cols <- sort(names(validValuesPerCol)[validValuesPerCol == max(as.numeric(names(table(validValuesPerCol))))])

for(n in names(accelerometers_cols)){
  accelerometers_cols[[n]] <- accelerometers_cols[[n]][accelerometers_cols[[n]] %in% valid_cols]
}

# reducing the columns to the bedefined set
# dim(pml_training_cleaned)
pml_training_cleaned <- pml_training_cleaned[c('classe',unlist(accelerometers_cols))]
pml_training_cleaned <- pml_training_cleaned[complete.cases(pml_training_cleaned),]
# dim(pml_training_cleaned)
classe_index_column <- 1

# converting classe column to factor
classes <- pml_training_cleaned$classe
classes <- as.factor(classes)
levels(classes) <- 1:length(levels(classes))
# classes <- as.numeric(classes)
pml_training_cleaned$classe <- classes


# test for near zero variance of columns
nzv_control <- nearZeroVar(pml_training_cleaned[,-1])

`%ni%` <- Negate(`%in%`)

```
First I quickly checked the intersection of columns of the training and test set just to get sure that we will not focus on feature that are not available in the test set. As aspected "classe" is missing in the test set. In addition the column "problem_id" is also unique.
Next I compared the different columns which belong to the four measuring instruments and defined a set of columns that are available for all four instruments.

In further check which rows contain how many valid values.

Finally, we got a `r dim(pml_training_cleaned)` data.frame containing 4*13 numeric columns and the 'classe' as factor.


#### Creation of Sets ####
```{r Creation to Training Sets}

 partition <- createDataPartition(y=pml_training_cleaned[,classe_index_column], p=0.75, list=FALSE)

trainingSet <- pml_training_cleaned[partition,]
testingSet <- pml_training_cleaned[partition,]
testing20cases <- pml_testing[,unlist(accelerometers_cols)]

# dim(trainingSet)
# dim(testingSet)
# dim(testing20cases)

```

Three sets were defined for the further processing. Training data were split in 75% train and 25% test data. The third data frame contains the to predict observations for the quiz.

Data.frames:

trainingSet: rows= `r dim(trainingSet)[1]`, columns= `r dim(trainingSet)[2]`

testingSet: rows= `r dim(testingSet)[1]`, columns= `r dim(testingSet)[2]`

testing20cases: rows= `r dim(testing20cases)[1]`, columns= `r dim(testing20cases)[2]`


#### Correlation ####
```{r Correlation, echo=TRUE, results='asis'}

featureCor <- abs(cor(subset(trainingSet,select = names(trainingSet) %ni% 'classe'),as.numeric(trainingSet[,classe_index_column])))

cat("In the histogram of correlation you can the that the features obviously only show weak correlation.")
h1  <- hist(featureCor, main = 'Correlation of features',xlab='Correlation')
show(h1)
bestCorrelations <- subset(as.data.frame(as.table(featureCor)), abs(Freq)>0.3)


coi <- rownames(featureCor)[which(featureCor > 0.20)]


cat("In a strip featurePlot we can see that all feature show the same distribution among the 5 classe's")

f1 <- featurePlot(x = subset(trainingSet,select = coi),
            y = trainingSet[,classe_index_column],
            plot = "strip"
)
show(f1)

cat("By the pairs featurePlot of the highest correlating features it is not possible to identify a clear feature for classification")
f2 <- featurePlot(x = subset(trainingSet,select = coi),
            y = trainingSet[,classe_index_column],
            plot = "pairs"
              
)
show(f2)

correlationMatrix <- cor(subset(trainingSet,select = names(trainingSet) %ni% 'classe'))
correlationMatrix <-  abs(correlationMatrix)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.8)
excludeColumns <- c(highlyCorrelated, classe_index_column)
cat("Here you can see the plot of correction")  
corrplot(correlationMatrix
        ,method=c("circle")
        ,type="lower"
        # ,order="hclust"
        ,hclust.method = "ward.D2"
        ,tl.cex=0.50
        ,tl.col="black"
        ,tl.srt = 45
        ,diag = F
        # ,cl.lim=c(-1,1)
         ,cl.lim=c(0,1)
        ,addgrid.col = NA
        )


# c2 <- corrplot(correlationMatrix[highlyCorrelated,highlyCorrelated]
#         ,method=c("circle")
#         ,type="lower"
#         ,order="hclust"
#         ,hclust.method = "ward.D2"
#         ,tl.cex=0.50
#         ,tl.col="black"
#         ,tl.srt = 45
#         ,diag = F
#         # ,cl.lim=c(-1,1)
#          ,cl.lim=c(0,1)
#         ,addgrid.col = NA
#         )
```

```{r}
df_temp <- subset(trainingSet,select = names(trainingSet) %ni% 'classe')
res.pca = PCA(X = df_temp
              ,scale.unit = FALSE #  is importent, data are not scale correctly
              ,ncp = ncol(df_temp) # number of final principal component
              ,graph = FALSE)

fv1 <- fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
fv2 <- fviz_pca_ind(res.pca, repel = TRUE, label="none", habillage = trainingSet$classe)
```

Also a reduction of dimantion by PCA doesn't allow us a good classifcation of data.

#### decision tree ####
```{r dTree}
# library('rpart')
# library('rpart.plot')
# dTreeModel <- rpart(classe ~ ., data = trainingSet, method= 'class')
# fancyRpartPlot(dTreeModel)

if(F){
dTreeModel <- train(classe ~ ., data = trainingSet, method= 'rpart')
saveRDS(dTreeModel,"dTreeModel.RDS")
}else{
  dTreeModel <- readRDS("dTreeModel.RDS")
}
# plot(dTreeModel$finalModel, uniform = TRUE, main="Classification Tree")
# text(dTreeModel$finalModel, use.n=TRUE, all= TRUE, cex=0.8)
fp <- fancyRpartPlot(dTreeModel$finalModel)
cat('Here you can see the result of the trained decision tree!')
show(fp)

dTreePrediction <- predict(dTreeModel$finalModel,testingSet,type='class')
dTreeValidation <-  confusionMatrix(dTreePrediction, testingSet$classe)
dTreeValidation
dTreeValidationAcc <- as.numeric(dTreeValidation$overall['Accuracy'])
cat('Howevery the model have only a weak accuracy (',dTreeValidationAcc,')')
```

#### random Forest ####
```{r random Forest}
if(F){
rForestModel_train <- train(classe ~ ., data = trainingSet, method= 'rf', prox=TRUE)#saveRDS(rForestModel,"rForestModel_train.RDS")
}else{
rForestModel_train <- readRDS("rForestModel_train.RDS")
}
rForestModel_train

rForest_trainPrediction <- predict(rForestModel_train,testingSet)
rForest_trainValidation <-  confusionMatrix(rForest_trainPrediction, testingSet$classe)

rForest_trainPrediction20cases <- predict(rForestModel_train,testing20cases)

rForest_trainValidationAcc <- rForestModel_train$results$Accuracy[rForestModel_train$results$mtry == as.numeric(rForestModel_train$bestTune)]

cat('The random forest model trained by the carrot::train function results in a accuracy of',rForest_trainValidationAcc,'.')
if(F){
rForestModel_randomFores <- randomForest(
  x=trainingSet[, -classe_index_column], 
  y=trainingSet$classe,
  xtest=testingSet[, -classe_index_column], 
  ytest=testingSet$classe, 
  ntree=100,
  keep.forest=TRUE,
  proximity=TRUE)
saveRDS(rForestModel_randomFores,"rForestModel_randomForest.RDS")
}else{
rForestModel_randomForest <- readRDS("rForestModel_randomForest.RDS")
}
rForest_randomForestPrediction <- predict(rForestModel_randomForest,testingSet)
rForest_randomForestValidation <-  confusionMatrix(rForest_randomForestPrediction, testingSet$classe)
rForest_randomForestPrediction20cases <- predict(rForestModel_randomForest,testing20cases)

rForestrForestModel_randomForestValidationAcc <- 1-sum(rForestModel_randomForest$confusion[,'class.error'])

cat('The random forest model trained by the randomForest::randomForest function results in a accuracy of',rForestrForestModel_randomForestValidationAcc,'.')

saveRDS(rForest_randomForestPrediction20cases,"rForest_randomForestPrediction.RDS") # all correct

```

#### Linear discriminant Analysis and naive Bayes ####
```{r Linear discriminant Analysis and naive Bayes}
if(F){
ldaModel <- train(classe ~ ., data = trainingSet, method= 'lda')
saveRDS(ldaModel,"ldaModel.RDS")
}else{
 ldaModel <- readRDS("ldaModel.RDS")
}
# nbModel <- train(classe ~ ., data = trainingSet, method= 'nb')
# saveRDS(nbModel,"nbModel.RDS")
if(F){
 trainCtrl <- trainControl(method   =   "repeatedcv",
                            repeats   =   3, 
                            number = 5, 
                            classProbs   =   TRUE, 
                            savePredictions = 'final',
                            summaryFunction   =   twoClassSummary) 

 colnames(trainingSet)
 temp_trainingSet <- trainingSet
 temp_trainingSet$classe <- as.factor(paste0('c',temp_trainingSet$classe))
 levels(temp_trainingSet$classe) <- paste0('c',levels(trainingSet$classe))
 nbModel <- train(classe ~ ., data = temp_trainingSet,
                        method   =   'nb',
                        # trControl   =   trainCtrl,
                        metric   =   "ROC", #for classification
                        preProc = c("center", "scale"),
                        tuneLength = 10) 
 nbModel <- train(classe ~ ., data = temp_trainingSet, method= 'nb')
 
  
  saveRDS(nbModel,"nbModel.RDS")
}else{
 nbModel <- readRDS("nbModel.RDS")
}

ldaPrediction <- predict(ldaModel,testingSet)
nbPrediction <- predict(nbModel,testingSet)
compareLdaNb <- table(ldaPrediction,nbPrediction)

ldaValidation <- confusionMatrix(ldaPrediction, testingSet$classe)

 temp_testingSet <- testingSet
 temp_testingSet$classe <- as.factor(paste0('c',temp_testingSet$classe))
 levels(temp_testingSet$classe) <- paste0('c',levels(testingSet$classe))
nbValidation <- confusionMatrix(nbPrediction, temp_testingSet$classe)

ldaModelValidationAcc <- ldaModel$results$Accuracy
nbModelValidationAcc <- nbModel$results$Accuracy[nbModel$results$usekernel==nbModel$bestTune$usekernel]

cat('The linear discriminant analysis Model results in an accuracy of',ldaModelValidationAcc,'and the model of naive bayes results in an accuracy of ',nbModelValidationAcc,'.')
```

#### Boosting ####
```{r Boosting}
if(F){
boostingModel <- train(classe ~ ., data = trainingSet, method= 'gbm', verbose=FALSE)
saveRDS(boostingModel,"boostingModel.RDS")
}else{
 boostingModel <- readRDS("boostingModel.RDS")
}
boostingPrediction <- predict(boostingModel,testingSet)
print(boostingModel)
boostingValidation <-  confusionMatrix(boostingPrediction, testingSet$classe)
boostingModel
boostingValidationAcc <- boostingModel$results$Accuracy[
  boostingModel$results$n.trees ==  boostingModel$bestTune$n.trees &
  boostingModel$results$interaction.depth ==  boostingModel$bestTune$interaction.depth
  ]
cat('The gradient boosting machine achieved an accuracy of',boostingValidationAcc,'.')
```

```{r}

accuracy_list <- c(
rForest_train = rForest_trainValidationAcc,
rForestrForestModel_randomForest = rForestrForestModel_randomForestValidationAcc,
lda = ldaModelValidationAcc,
nb = nbModelValidationAcc,
boosting = boostingValidationAcc
)

accuracy_list < sort(accuracy_list, decreasing = T)
cat('The random Forest Model performed best of all tested methods.')

accuracy_list[1]

letters <- c('A','B','C','D','E')
final_prediction <- letters[rForest_trainPrediction20cases]
cat('For the quiz the predicted classes are:',final_prediction)

```


